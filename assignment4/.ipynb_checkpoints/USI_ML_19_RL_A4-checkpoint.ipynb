{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1kKJk_rJC34"
   },
   "source": [
    "# Machine Learning 2019/2020: Assignment 4 -  Reinforcement Learning\n",
    "Deadline: Friday 6th of December 2019 9pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DLtJ0vlKJReh"
   },
   "source": [
    "First name: Brian  \n",
    "Last name: Pulfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8xajf5MJ8Ud"
   },
   "source": [
    "## About this assignment\n",
    "\n",
    "In this assignment you will further deepen your understanding of Reinforcement Learning (RL).\n",
    "\n",
    "## Submission instructions\n",
    "\n",
    "Please write your answers, equations, and code directly in this python notebook and print the final result to pdf (File > Print).\n",
    "Make sure that code has appropriate line breaks such that all code is visible in the final pdf.\n",
    "Also select A3 for the PDF size to prevent content from being clipped.\n",
    "\n",
    "The final pdf must be named name.lastname.pdf and uploaded to the iCorsi website before the deadline expires. Late submissions will result in 0 points.\n",
    "\n",
    "**Also share this notebook (top right corner 'Share') with teaching.idsia@gmail.com during submission.**\n",
    "\n",
    "**Keep your answers brief and respect the sentence limits in each question (answers exceeding the limit are not taken into account)**.\n",
    "\n",
    "Learn more about python notebooks and formatting here: https://colab.research.google.com/notebooks/welcome.ipynb\n",
    "\n",
    "## How to get help\n",
    "\n",
    "We encourage you to use the tutorials to ask questions or to discuss exercises with other students.\n",
    "However, do not look at any report written by others or share your report with others. Violation of that rule will result in 0 points for all students involved. For further questions you can send an email to louis@idsia.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3uo_XEzLkxc"
   },
   "source": [
    "## 1 Basic probability (6p)\n",
    "\n",
    "Suppose that a migrating lizard that rests in Ticino can be in four different states:\n",
    "Eating (E), Sleeping (S), Fighting (F) and Mating (M), for example protecting its territory against other lizards. Each lizard spends 30% of its time sleeping, 40% eating, 20% fighting and the remaining time mating. A biologist collects a population of lizards and puts them in a cage to study their behaviors. Suppose the probability for a lizard being caught while eating is 0.1, for a sleeping lizard 0.4, for a fighting lizard 0.8 and for the lizards that are mating 0.2, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRg4g3_NMXVI"
   },
   "source": [
    "### Question 1.1 (3p)\n",
    "What is the relative frequency (probability) for a lizard being caught in the cage?\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "0.4\\*0.1 + 0.3\\*0.4 + 0.2\\*0.8 + 0.1\\*0.2 = 0.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OO7HpEwMdnH"
   },
   "source": [
    "### Question 1.2 (3p)\n",
    "\n",
    "What is the proportion of lizards that are fighting of those that were caught in the cage?\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "0.2\\*0.8 / 0.34 ≈ 0.4705"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okJpynzHMpA6"
   },
   "source": [
    "## 2 Markov Decision Processes (32p)\n",
    "\n",
    "Suppose a robot is put in a maze with long corridor. The\n",
    "corridor is 1 kilometer long and 5 meters wide. The available actions to the robot are moving forward for 1 meter, moving backward for 1 meter, turning left for 90 degrees and turning right for 90 degrees. If the robot moves and hits the wall, then it will stay in its position and orientation. The robot's goal is to escape from this maze by reaching the end of the long corridor.\n",
    "**Note: the answers in the following questions should not exceed 5 sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rI3AATMANzC_"
   },
   "source": [
    "### Question 2.1 (4p)\n",
    "\n",
    "Assume the robot receives a +1 reward signal for each time step taken in the\n",
    "maze and +1000 for reaching the final goal (the end of the long corridor). Then you train the robot for a while, but it seems it still does not perform well at all for navigating to the end of the corridor in the maze. What is happening? Is there something wrong with the reward function?\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "What is happening is that the biggest reward is given to the robot only after a very long sequence of actions. Since the initial behaviour of the robot is random, he will hardly ever find out that there is a much bigger reward of +1'000 at the end of the corridor. Instead, the robot might spend all it's time recieving +1 rewards simply staying in the corridor.\n",
    "\n",
    "The reward function should not incourage the robot staying in the corridor as its doing (by giving +1 rewards at each timesteps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LC-aYmiFOAEZ"
   },
   "source": [
    "### Question 2.2 (4p)\n",
    "\n",
    "If there is something wrong with the reward function, how could you fix it? If not, how to resolve the training issues?\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "Instead of giving a +1 reward at each timestep (incouraging the robot staying in the corridor), we could give a +1 reward if the robot rotates/moves towards the closest exit of the corridor with respect to his position and give a -1 reward if the robot rotates/moves towards the further exit or the wall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzIDv8qoOGuH"
   },
   "source": [
    "### Questions 2.3 (2p)\n",
    "\n",
    "The discounted return for a non-episodic task is defined as\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n",
    "$$\n",
    "where $\\gamma \\in [0, 1]$ is the discount factor.\n",
    "\n",
    "Rewrite the above equation such that $G_t$ is on the left hand side and $G_{t+1}$ is on the right hand side.\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "The above fomula can be rewritten (in terms of $G_t$ and $G_{t+1}$) as:\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma G_{t+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTxEimD5O5eA"
   },
   "source": [
    "### Questions 2.4 (2p)\n",
    "\n",
    "What is the sufficient condition for this infinite series to be a convergent series?\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "The sufficient condition for the series to converge is:\n",
    "\n",
    "$$\n",
    "0 \\le \\gamma <1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bp30j2csPJkz"
   },
   "source": [
    "### Questions 2.5 (5p)\n",
    "\n",
    "Suppose this infinite series is a convergent series, and each reward in the series is a constant of +1. We know the series is bounded, what is a simple formula for this bound ? Write it down without using summation.\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "$$\n",
    "bound = 1 + \\frac{\\gamma}{1-\\gamma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmDQKWHNPSnx"
   },
   "source": [
    "### Questions 2.6 (5p)\n",
    "\n",
    "Let the task be an episodic setting and the robot is running for $T = 5$ time steps. Suppose $\\gamma = 0.3$, and the robot receives rewards along the way $R_1 = −1, R_2 = −0.5, R_3 = 2, R_4 = 1, R_5 = 6$. What are the values for $G_0, G_1, G_2, G_3, G_4, G_5$?\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "$$\n",
    "G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + \\gamma^4 R_5 = - 0.8944\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_1 = R_2 + \\gamma R_3 + \\gamma^2 R_4 + \\gamma^3 R_5 = 0.352\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_2 = R_3 + \\gamma R_4 + \\gamma^2 R_5 = 2.84\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_3 = R_4 + \\gamma R_5 = 2.8\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_4 = R_5 = 6\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_5 = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7ZcAEvfQA8f"
   },
   "source": [
    "### Questions 2.7 (5p)\n",
    "\n",
    "Suppose each reward in the series is increased by a constant $c$, i.e. $R_t \\leftarrow R_t + c$.\n",
    "Then how does it change $G_t$?\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "Supposing the episode has T timesteps, $G_t$ was previously defined as:\n",
    "$$\n",
    "G_t = \\sum_{j=0}^{\\infty} \\gamma^j R_{t+1+j} \n",
    "$$\n",
    "\n",
    "After adding a constant c to every reward, $G_t$ becomes:\n",
    "$$\n",
    "G_t = \\sum_{j=0}^{\\infty} \\gamma^j (R_{t+1+j} + c)\n",
    "$$\n",
    "\n",
    "The total value of $G_t$ increases for any non-negative constant c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7KPVdhPQBpz"
   },
   "source": [
    "### Questions 2.8 (5p)\n",
    "\n",
    "Now consider episodic tasks, and similar to Question 2.7, we add a constant $c$ to each reward, how does it change $G_t$?\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "Supposing the episode has T timesteps, $G_t$ was previously defined as:\n",
    "$$\n",
    "G_t = \\sum_{j=0}^{T-t} \\gamma^j R_{t+1+j} \n",
    "$$\n",
    "\n",
    "After adding a constant c to every reward, $G_t$ becomes:\n",
    "$$\n",
    "G_t = \\sum_{j=0}^{T-t} \\gamma^j (R_{t+1+j} + c)\n",
    "$$\n",
    "\n",
    "The total value of $G_t$ increases for any non-negative constant c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjOnM7AVQVOB"
   },
   "source": [
    "## 3 Dynamic Programming (62p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Jm0EyIHQaOp"
   },
   "source": [
    "### Questions 3.1 (5p)\n",
    "\n",
    "Write down the Bellman optimality equation for the state value function without using expectation notation, but using probability distributions instead. \n",
    "Define all variables and probability distributions in bullet points.\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "$$\n",
    "V^\\pi(s)=\\sum_a\\pi(s,a) (\\sum_{s'} \\gamma V^\\pi(s') \\sum_{r'} r'P(s', r' | a, s))\n",
    "$$\n",
    "\n",
    "* $\\pi$(s,a) = Probability of doing action a in state s according to policy $\\pi$\n",
    "* $\\gamma$ = Discount factor ($0 \\le \\gamma < 1$)\n",
    "* $P(s', r' | a, s)$ = Probability of getting reward r' and transitioning to state s' given that we take action a in state s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0VZcK6LQkUC"
   },
   "source": [
    "### Questions 3.2 (5p)\n",
    "\n",
    "Write down the Bellman optimality equation for the state-action value function without using expectation notation, but using probability distributions instead.\n",
    "Define all variables and probability distributions in bullet points.\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "$$\n",
    "Q^\\pi(s, a) = \\sum_{s'}\\gamma V(s') \\sum_{r'} P(s', r' | s, a) + \\sum_{r'} r'P(s', r' | a, s)\n",
    "$$\n",
    "\n",
    "* $V(s')$ = Value function for state s\n",
    "* $P(s', r' | s, a)$ = Probability of transitioning to state s' and get reward r' given that we take action a in state s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tq66sRJeQlE2"
   },
   "source": [
    "### Questions 3.3 (15p)\n",
    "\n",
    "Consider a 4x4 gridworld depicted in the following table:\n",
    "\n",
    "![Grid world](https://i.ibb.co/HdSdKJB/image.png)\n",
    "\n",
    "The non-terminal states are $S = \\{1, 2, \\ldots, 14\\}$ and the terminal states are $\\bar S = \\{0, 15\\}$.\n",
    "There are four available actions for each state, that is $A = \\{\\text{up}, \\text{down}, \\text{left}, \\text{right}\\}$.\n",
    "Assume the state transitions are deterministic and all transitions result in a negative reward −1 (after termination all rewards are zero).\n",
    "If the agent hits the boundary, then its state will remain unchanged, e.g. $p(s=8, r=−1|s=8, a=\\text{left}) = 1$.\n",
    "Note: In this exercise, we assume the policy is a deterministic\n",
    "function.\n",
    "\n",
    "Manually run the policy iteration algorithm (see lecture slide 58) for one iteration. Use the in-place policy iteration algorithm.\n",
    "This means one time policy evaluation with a single pass through the states (16 equations) and one time policy improvement.\n",
    "Assume the initial state value for all 16 cells are 0.0 and the policy initially always outputs the 'left' action.\n",
    "Write down the equations and detailed numerical computations for the updated values of each cell.\n",
    "Use a discount factor $\\gamma = 0.5$.\n",
    "Write down the policy after policy improvement.\n",
    "\n",
    "![Policy iteration](http://www.incompleteideas.net/book/ebook/imgtmp5.png)\n",
    "\n",
    "Read more about this in Sutton & Barto's book http://www.incompleteideas.net/book/ebook/node43.html\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "***Policy evaluation*** \n",
    "$$ V(0) = 0 $$\n",
    "$$V(1) \\leftarrow -1 + \\gamma V(0) = -1 + 0 = -1$$\n",
    "$$V(2) \\leftarrow -1 + \\gamma V(1) = -1.5$$\n",
    "$$V(3) \\leftarrow -1 + \\gamma V(2) = -1.75 $$\n",
    "$$V(4) \\leftarrow -1 + \\gamma V(4) = -1$$\n",
    "$$V(5) \\leftarrow -1 + \\gamma V(4) = -1.5$$\n",
    "$$V(6) \\leftarrow -1 + \\gamma V(5) = -1.75$$\n",
    "$$V(7) \\leftarrow -1 + \\gamma V(6) = -1.875 $$\n",
    "$$V(8) \\leftarrow -1 + \\gamma V(8) = -1$$\n",
    "$$V(9) \\leftarrow -1 + \\gamma V(8) = -1.5$$\n",
    "$$V(10) \\leftarrow -1 + \\gamma V(9) = -1.75$$\n",
    "$$V(11) \\leftarrow -1 + \\gamma V(10) = -1.875 $$\n",
    "$$V(12) \\leftarrow -1 + \\gamma V(12) = -1$$\n",
    "$$V(13) \\leftarrow -1 + \\gamma V(12) = -1.5$$\n",
    "$$V(14) \\leftarrow -1 + \\gamma V(13) = -1.75$$\n",
    "$$ V(15) = 0$$\n",
    "\n",
    "\n",
    "\n",
    "***Policy improvement***\n",
    "It is enough, for every state, to compute the $argmax(left, up, right, down)$. Each term in the $argmax_a$ function is the sum of the reward of transitioning from the current state to summed with the discounted value from that next state. \n",
    "\n",
    "$$\n",
    "\\pi '(1) = argmax_a(-1 + \\gamma V^\\pi(0), -1 + \\gamma V^\\pi(1), -1 + \\gamma V^\\pi(2), -1 + \\gamma V^\\pi(5)) = left\n",
    "$$\n",
    "$$\n",
    "\\pi '(2) = argmax_a(-1 + \\gamma V^\\pi(1), -1 + \\gamma V^\\pi(2), -1 + \\gamma V^\\pi(3), -1 + \\gamma V^\\pi(6)) = left\n",
    "$$\n",
    "$$\n",
    "\\pi '(3) = argmax_a(-1 + \\gamma V^\\pi(2), -1 + \\gamma V^\\pi(3), -1 + \\gamma V^\\pi(3), -1 + \\gamma V^\\pi(7)) = left\n",
    "$$\n",
    "$$\n",
    "\\pi '(4) = argmax_a(-1 + \\gamma V^\\pi(4), -1 + \\gamma V^\\pi(0), -1 + \\gamma V^\\pi(5), -1 + \\gamma V^\\pi(8)) = up\n",
    "$$\n",
    "$$\n",
    "\\pi '(5) = argmax_a(-1 + \\gamma V^\\pi(4), -1 + \\gamma V^\\pi(1), -1 + \\gamma V^\\pi(6), -1 + \\gamma V^\\pi(9)) = left / up\n",
    "$$\n",
    "$$\n",
    "\\pi '(6) = argmax_a(-1 + \\gamma V^\\pi(5), -1 + \\gamma V^\\pi(2), -1 + \\gamma V^\\pi(7), -1 + \\gamma V^\\pi(10)) = left / up\n",
    "$$\n",
    "$$\n",
    "\\pi '(7) = argmax_a(-1 + \\gamma V^\\pi(6), -1 + \\gamma V^\\pi(3), -1 + \\gamma V^\\pi(7), -1 + \\gamma V^\\pi(11)) = left / up\n",
    "$$\n",
    "$$\n",
    "\\pi '(8) = argmax_a(-1 + \\gamma V^\\pi(8), -1 + \\gamma V^\\pi(4), -1 + \\gamma V^\\pi(9), -1 + \\gamma V^\\pi(12)) = up\n",
    "$$\n",
    "$$\n",
    "\\pi '(9) = argmax_a(-1 + \\gamma V^\\pi(8), -1 + \\gamma V^\\pi(5), -1 + \\gamma V^\\pi(10), -1 + \\gamma V^\\pi(13)) = left\n",
    "$$\n",
    "$$\n",
    "\\pi '(10) = argmax_a(-1 + \\gamma V^\\pi(9), -1 + \\gamma V^\\pi(6), -1 + \\gamma V^\\pi(11), -1 + \\gamma V^\\pi(14)) = left\n",
    "$$\n",
    "$$\n",
    "\\pi '(11) = argmax_a(-1 + \\gamma V^\\pi(10), -1 + \\gamma V^\\pi(7), -1 + \\gamma V^\\pi(11), -1 + \\gamma V^\\pi(15)) = down \n",
    "$$\n",
    "$$\n",
    "\\pi '(12) = argmax_a(-1 + \\gamma V^\\pi(12), -1 + \\gamma V^\\pi(8), -1 + \\gamma V^\\pi(13), -1 + \\gamma V^\\pi(12)) = left / up\n",
    "$$\n",
    "$$\n",
    "\\pi '(13) = argmax_a(-1 + \\gamma V^\\pi(12), -1 + \\gamma V^\\pi(9), -1 + \\gamma V^\\pi(14), -1 + \\gamma V^\\pi(13)) = left\n",
    "$$\n",
    "$$\n",
    "\\pi '(14) = argmax_a(-1 + \\gamma V^\\pi(13), -1 + \\gamma V^\\pi(10), -1 + \\gamma V^\\pi(15), -1 + \\gamma V^\\pi(14)) = right\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ciourK1wQo-3"
   },
   "source": [
    "### Questions 3.4 (15p)\n",
    "\n",
    "Implement the beforementioned environment in the code skeleton below.\n",
    "Come up with your own solution and do not copy the code from a third party source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3gmuwlmdyO4"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YkzicBh-I3dU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "np.set_printoptions(precision=3, linewidth=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DT_bNpgqd2mM"
   },
   "source": [
    "#### Defining the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNBoBp3PJC0C"
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "  UP = 0\n",
    "  DOWN = 1\n",
    "  LEFT = 2\n",
    "  RIGHT = 3\n",
    "\n",
    "  def __init__(self, side=4):\n",
    "    self.side = side\n",
    "    # -------------------------\n",
    "    # Define integer states, actions, and final states as specified in the problem description\n",
    "\n",
    "    # TODO insert code here\n",
    "    self.actions = [self.UP, self.DOWN, self.LEFT, self.RIGHT]\n",
    "    self.states = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "    self.finals = [0, 15]\n",
    "\n",
    "    # -------------------------\n",
    "    self.actions_repr = np.array(['↑', '↓', '←', '→'])\n",
    "\n",
    "  def reward(self, s, s_next, a):\n",
    "    # -------------------------\n",
    "    # Return the reward for the given transition as specified in the problem description\n",
    "\n",
    "    # TODO insert code here\n",
    "    if s in self.finals:\n",
    "        return 0\n",
    "    return -1\n",
    "    # -------------------------\n",
    "\n",
    "  def transition_prob(self, s, s_next, a):\n",
    "    # -------------------------\n",
    "    # Return a probability in [0, 1] for the given transition as specified in the problem description\n",
    "\n",
    "    # TODO insert code here\n",
    "    states_matrix = [[0, 1, 2, 3],\n",
    "                     [4, 5, 6, 7],\n",
    "                     [8, 9, 10, 11],\n",
    "                     [12, 13, 14, 15]]\n",
    "    \n",
    "    actual_next = -1\n",
    "    \n",
    "    for row_index in range(len(states_matrix)):\n",
    "        for col_index in range(len(states_matrix[row_index])):\n",
    "            if s == states_matrix[row_index][col_index]:\n",
    "                if a == self.UP:\n",
    "                    if row_index -1 >= 0:\n",
    "                        actual_next = states_matrix[row_index-1][col_index]\n",
    "                    else:\n",
    "                        actual_next = states_matrix[row_index][col_index]\n",
    "                elif a == self.DOWN:\n",
    "                    if row_index +1 < 4:\n",
    "                        actual_next = states_matrix[row_index+1][col_index]\n",
    "                    else:\n",
    "                        actual_next = states_matrix[row_index][col_index]\n",
    "                elif a == self.LEFT:\n",
    "                    if col_index -1 >= 0:\n",
    "                        actual_next = states_matrix[row_index][col_index-1]\n",
    "                    else:\n",
    "                        actual_next = states_matrix[row_index][col_index]\n",
    "                elif a == self.RIGHT:\n",
    "                    if col_index +1 < 4:\n",
    "                        actual_next = states_matrix[row_index][col_index+1]\n",
    "                    else:\n",
    "                        actual_next = states_matrix[row_index][col_index]\n",
    "                else:\n",
    "                    print(\"Impossible action specified!\")\n",
    "                \n",
    "                if actual_next == s_next:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "    return 0\n",
    "    # -------------------------\n",
    "\n",
    "  def print_policy(self, policy):\n",
    "    P = np.array(policy).reshape(self.side, self.side)\n",
    "    print(self.actions_repr[P])\n",
    "  \n",
    "  def print_values(self, values):\n",
    "    V = np.array(values).reshape(self.side, self.side)\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqMt-0yAuGz8"
   },
   "source": [
    "### Questions 3.5 (17p)\n",
    "\n",
    "Implement policy iteration in the code skeleton below.\n",
    "Come up with your own solution and do not copy the code from a third party source.\n",
    "\n",
    "Run the code multiple times. Do you always end up with the same policy? Why? (max 4 sentences)\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "No, we don't always end up with the same policy.\n",
    "This is due to the fact that multiple optimal policies exists and to the fact that the initial behaviour is randomly picked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PrUMxh-qd5u0"
   },
   "source": [
    "#### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m1DOXcH5J0NR"
   },
   "outputs": [],
   "source": [
    "def eval_policy(world, policy, values, gamma=0.9, theta=0.01):\n",
    "  # --------------------------\n",
    "  # Implement policy evaluation and return the updated value function\n",
    "\n",
    "  # TODO insert code here\n",
    "    delta = -1\n",
    "    \n",
    "    while delta > theta or delta == -1:\n",
    "        for state in range(len(world.states)):\n",
    "            action = policy[state]\n",
    "\n",
    "            partial_sum = 0\n",
    "            delta = 0\n",
    "            for n_state in range(len(world.states)):\n",
    "                reward = world.reward(state, n_state, action)\n",
    "                p_a_ss = world.transition_prob(state, n_state, action)\n",
    "                partial_sum = partial_sum + p_a_ss * (reward + gamma*values[n_state])\n",
    "\n",
    "            delta = max(delta, abs(values[state]-partial_sum))\n",
    "            values[state] = partial_sum\n",
    "    return values\n",
    "  # --------------------------\n",
    "\n",
    "\n",
    "def improve_policy(world, policy, values, gamma=0.9):\n",
    "  # --------------------------\n",
    "  # Implement policy improvement and return the updated policy\n",
    "\n",
    "  # TODO insert code here\n",
    "    stability = True\n",
    "    new_policy = [-1] * len(policy)\n",
    "\n",
    "    for state in world.states:\n",
    "        best_action_value, best_action = -1000000, -1\n",
    "        \n",
    "        for action in world.actions:\n",
    "            current_action_value = 0\n",
    "            \n",
    "            for n_state in world.states:\n",
    "                reward = world.reward(state, n_state, action)\n",
    "                probability = world.transition_prob(state, n_state, action)\n",
    "                current_value = probability * (reward + gamma*values[n_state])\n",
    "                \n",
    "                current_action_value = current_action_value + current_value\n",
    "            \n",
    "            if current_action_value > best_action_value:\n",
    "                best_action_value = current_action_value\n",
    "                best_action = action\n",
    "                \n",
    "        new_policy[state] = best_action\n",
    "        \n",
    "        if new_policy[state] != policy[state]:\n",
    "            stability = False\n",
    "    \n",
    "    # Policy update\n",
    "    for index in range(len(policy)):\n",
    "        policy[index] = new_policy[index]\n",
    "    \n",
    "    return stability\n",
    "  # --------------------------\n",
    "\n",
    "\n",
    "def policy_iteration(world, gamma=0.9, theta=0.01):\n",
    "  # Initialize a random policy\n",
    "  policy = np.array([np.random.choice(world.actions) for s in world.states])\n",
    "  print('Initial policy')\n",
    "  world.print_policy(policy)\n",
    "  # Initialize values to zero\n",
    "  values = np.zeros_like(world.states, dtype=np.float32)\n",
    "\n",
    "  # Run policy iteration\n",
    "  stable = False\n",
    "  for i in itertools.count():\n",
    "    print(f'Iteration {i}')\n",
    "    values = eval_policy(world, policy, values, gamma, theta)\n",
    "    world.print_values(values)\n",
    "    stable = improve_policy(world, policy, values, gamma)\n",
    "    world.print_policy(policy)\n",
    "    if stable:\n",
    "      break\n",
    "\n",
    "  return policy, values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqdoNw97mcEA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial policy\n",
      "[['→' '→' '↓' '→']\n",
      " ['↓' '←' '→' '↓']\n",
      " ['←' '↓' '←' '↑']\n",
      " ['→' '↑' '←' '↑']]\n",
      "Iteration 0\n",
      "[[-0.938 -1.969 -1.984 -1.938]\n",
      " [-1.938 -1.969 -1.992 -1.996]\n",
      " [-1.938 -1.996 -1.998 -1.998]\n",
      " [-1.996 -1.998 -1.999 -0.999]]\n",
      "[['↑' '←' '→' '↑']\n",
      " ['↑' '←' '←' '↑']\n",
      " ['↑' '←' '↑' '↓']\n",
      " ['↑' '↑' '→' '↓']]\n",
      "Iteration 1\n",
      "[[-0.007 -1.004 -2.    -2.   ]\n",
      " [-1.004 -1.502 -1.751 -2.   ]\n",
      " [-1.502 -1.751 -1.875 -1.008]\n",
      " [-1.751 -1.875 -1.008 -0.008]]\n",
      "[['↑' '←' '←' '↑']\n",
      " ['↑' '↑' '←' '↓']\n",
      " ['↑' '↑' '↓' '↓']\n",
      " ['↑' '→' '→' '↓']]\n",
      "Iteration 2\n",
      "[[-0.004 -1.002 -1.501 -2.   ]\n",
      " [-1.002 -1.501 -1.75  -1.504]\n",
      " [-1.501 -1.75  -1.504 -1.004]\n",
      " [-1.75  -1.504 -1.004 -0.004]]\n",
      "[['↑' '←' '←' '←']\n",
      " ['↑' '↑' '↑' '↓']\n",
      " ['↑' '↑' '↓' '↓']\n",
      " ['↑' '→' '→' '↓']]\n",
      "Iteration 3\n",
      "[[-0.002 -1.001 -1.5   -1.75 ]\n",
      " [-1.001 -1.5   -1.75  -1.502]\n",
      " [-1.5   -1.75  -1.502 -1.002]\n",
      " [-1.75  -1.502 -1.002 -0.002]]\n",
      "[['↑' '←' '←' '←']\n",
      " ['↑' '↑' '↑' '↓']\n",
      " ['↑' '↑' '↓' '↓']\n",
      " ['↑' '→' '→' '↓']]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your code, please include the output in your submission\n",
    "world = GridWorld()\n",
    "policy, values = policy_iteration(world, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hRFm6Zm6YI1"
   },
   "source": [
    "### Questions 3.5 (5p)\n",
    "\n",
    "Let's run policy iteration with $\\gamma = 1$. Describe what is happening. Why is this the case? Give an example. What is $\\gamma$ trading off and how does it affect policy iteration? (max 8 sentences)\n",
    "\n",
    "---\n",
    "\n",
    "**ANSWER HERE**\n",
    "\n",
    "For $\\gamma = 1$, the policy evaluation becomes the expected reward that we get taking an action in a state plus the exact same thing for the state we get into. This translates into having a policy that simply counts how many squares away (considering we move ) is the current state from a terminal state.\n",
    "\n",
    "The series in this case converges because after visiting a final state, the next rewards are all zeroes (in the implementation, the probability of transitioning from a final state to any other state is 0).\n",
    "\n",
    "\n",
    "The trade off with gamma is the following: Higher values of gamma allow to create a behaviour that 'thinks in the long run', while with lower values of gamma we create an agent that cares most about the immediate rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rAQ1K_u6qtH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial policy\n",
      "[['←' '↑' '↓' '↑']\n",
      " ['→' '←' '↓' '→']\n",
      " ['↓' '↓' '↓' '↓']\n",
      " ['↓' '←' '↓' '→']]\n",
      "Iteration 0\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -2. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -2. -1.  0.]]\n",
      "[['↑' '←' '↑' '↑']\n",
      " ['↑' '↑' '↑' '↑']\n",
      " ['↑' '←' '↑' '↓']\n",
      " ['↑' '↑' '→' '↓']]\n",
      "Iteration 1\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -3. -3.]\n",
      " [-2. -3. -4. -1.]\n",
      " [-3. -4. -1.  0.]]\n",
      "[['↑' '←' '←' '↑']\n",
      " ['↑' '↑' '↑' '↓']\n",
      " ['↑' '↑' '↓' '↓']\n",
      " ['↑' '→' '→' '↓']]\n",
      "Iteration 2\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "[['↑' '←' '←' '↓']\n",
      " ['↑' '↑' '↑' '↓']\n",
      " ['↑' '↑' '↓' '↓']\n",
      " ['↑' '→' '→' '↓']]\n",
      "Iteration 3\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "[['↑' '←' '←' '↓']\n",
      " ['↑' '↑' '↑' '↓']\n",
      " ['↑' '↑' '↓' '↓']\n",
      " ['↑' '→' '→' '↓']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 2, 2, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 3, 3, 1]),\n",
       " array([ 0., -1., -2., -3., -1., -2., -3., -2., -2., -3., -2., -1., -3., -2., -1.,  0.], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_iteration(world, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-3gmuwlmdyO4"
   ],
   "name": "USI ML 19 RL A4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
